{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/'\n",
    "data_config = pickle.load(open(base_dir + 'data_config.dict', 'rb'))\n",
    "MAX_LEN = data_config['MAX_LEN']\n",
    "WORD_DIM = data_config['WORD_DIM']\n",
    "RELATION_NUM = data_config['RELATION_NUM']\n",
    "POS_MIN = -100\n",
    "POS_EMBED_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22549, 300) 22548\n"
     ]
    }
   ],
   "source": [
    "word_embed = pickle.load(open(base_dir + 'word_embed', 'rb'))\n",
    "word_embed = np.transpose(word_embed)\n",
    "PAD_ID = word_embed.shape[0]-1\n",
    "print(word_embed.shape, PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2lb = pickle.load(open(base_dir + 'rel2lb.dict', 'rb'))\n",
    "OTHER_LABEL = rel2lb['Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from tfrecord\n",
    "\n",
    "def pad_fixed_length(words):\n",
    "    words = tf.pad(words,tf.constant([[0, MAX_LEN-words.shape[0]]]), constant_values=PAD_ID)\n",
    "    return words\n",
    "\n",
    "def processing(raw):\n",
    "    features = tf.io.parse_single_example(\n",
    "        raw,\n",
    "        features={\n",
    "            'idxs': tf.io.FixedLenFeature([2], tf.int64),\n",
    "            'label': tf.io.FixedLenFeature([1], tf.int64),\n",
    "            'words': tf.io.VarLenFeature(tf.int64)\n",
    "        }\n",
    "    )\n",
    "    idxs = tf.cast(features['idxs'], tf.int32)\n",
    "    pos1 = tf.range(0, MAX_LEN, 1, dtype=tf.int32) - idxs[0]\n",
    "    pos2 = tf.range(0, MAX_LEN, 1, dtype=tf.int32) - idxs[1]\n",
    "    pos1 = pos1 - POS_MIN\n",
    "    pos2 = pos2 - POS_MIN\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    words = tf.cast(tf.sparse.to_dense(features['words']), tf.int32)\n",
    "    words = tf.py_function(pad_fixed_length, [words], Tout=tf.int32)\n",
    "    return pos1, pos2, label, words\n",
    "\n",
    "train_ds = tf.data.TFRecordDataset(filenames = [base_dir + 'train.tfrecords']).map(processing).shuffle(2000).batch(128)\n",
    "test_ds = tf.data.TFRecordDataset(filenames = [base_dir + 'test.tfrecords']).map(processing).shuffle(2000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(keras.Model):\n",
    "    def __init__(self, word_embed, pos_dim, word_dim, class_num, class_dim, other_label, hyperparams):\n",
    "        super(Network, self).__init__()\n",
    "        self.pos_dim = pos_dim\n",
    "        self.word_dim = word_dim\n",
    "        self.other_label = tf.constant(other_label, dtype=tf.int32)\n",
    "        self.m_neg = tf.constant(hyperparams['m_neg'])\n",
    "        self.m_pos = tf.constant(hyperparams['m_pos'])\n",
    "        self.gamma = tf.constant(hyperparams['gamma'])\n",
    "        self.beta = tf.constant(hyperparams['beta'])\n",
    "        self.word_embed = tf.Variable(word_embed, dtype=tf.float32)\n",
    "        self.pos1_embed = tf.Variable(tf.random.uniform([POS_EMBED_LEN, pos_dim],minval=0,maxval=1), dtype=tf.float32)\n",
    "        self.pos2_embed = tf.Variable(tf.random.uniform([POS_EMBED_LEN, pos_dim],minval=0,maxval=1), dtype=tf.float32)\n",
    "        class_embed_init_param = tf.sqrt(6/(class_num + class_dim))\n",
    "        self.class_matrix = tf.Variable(tf.random.uniform([class_dim, class_num],minval=-class_embed_init_param, maxval=class_embed_init_param), dtype=tf.float32) \n",
    "        \n",
    "        \n",
    "        ctx1 = np.zeros([MAX_LEN, MAX_LEN])\n",
    "        for i in range(0,MAX_LEN-1):\n",
    "            ctx1[i+1, i] = 1\n",
    "            \n",
    "        ctx2 = np.zeros([MAX_LEN, MAX_LEN])\n",
    "        for i in range(1,MAX_LEN):\n",
    "            ctx2[i-1, i] = 1\n",
    "        \n",
    "        self.ctx_mat1 = tf.constant(ctx1, dtype=tf.float32)\n",
    "        self.ctx_mat2 = tf.constant(ctx2, dtype=tf.float32)\n",
    "        \n",
    "        self.conv1 = layers.Conv1D(class_dim, 1, padding='same', activation=tf.nn.tanh)\n",
    "        self.pool1 = layers.MaxPool1D(MAX_LEN, padding='same')\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def other_loss_func(score, m_neg, gamma):\n",
    "        return tf.math.log(1.0 + tf.exp(gamma*(m_neg + tf.reduce_max(score, axis=1))))\n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def remove_ele(data):\n",
    "        label = tf.cast(data[-1], dtype=tf.int32)\n",
    "        row = data[:-1]\n",
    "        return tf.concat([row[:label],row[label+1:]], axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def class_loss_func(score, class_label, m_neg, m_pos, gamma):\n",
    "        \n",
    "        first_term = tf.math.log(1.0 + tf.exp(gamma*(m_pos - tf.gather_nd(score, tf.expand_dims(class_label, axis=1), batch_dims=1 ))))\n",
    "        first_term = tf.squeeze(first_term, axis=1)\n",
    "        score_temp = tf.map_fn(Network.remove_ele, tf.concat([score, tf.cast(class_label, dtype=tf.float32)], axis=1), dtype=tf.float32)\n",
    "        second_term = tf.math.log(1.0 + tf.exp(gamma*(m_neg + tf.reduce_max(score_temp, axis=1))))\n",
    "        return first_term + second_term\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        pos1 = inputs[0]\n",
    "        pos2 = inputs[1]\n",
    "        label = inputs[2]\n",
    "        words = inputs[3]\n",
    "        pf1 = tf.nn.embedding_lookup(self.pos1_embed, pos1)\n",
    "        pf2 = tf.nn.embedding_lookup(self.pos2_embed, pos2)\n",
    "        wf = tf.nn.embedding_lookup(self.word_embed, words)\n",
    "        wf = tf.concat([pf1, pf2, wf], axis=2)\n",
    "        wf_before = tf.matmul(self.ctx_mat1, wf)\n",
    "        wf_follow = tf.matmul(self.ctx_mat2, wf)\n",
    "        wf_context = tf.concat([wf_before, wf, wf_follow], axis=2)\n",
    "        \n",
    "        x = self.conv1(wf_context)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = tf.matmul(x, self.class_matrix)\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "        \n",
    "        if training:\n",
    "            other_mask = tf.squeeze(tf.equal(label, self.other_label), axis=1)\n",
    "            class_mask = tf.math.logical_not(other_mask)\n",
    "            other_scores = tf.boolean_mask(x, other_mask)\n",
    "            class_scores = tf.boolean_mask(x, class_mask)\n",
    "            class_label = tf.boolean_mask(label, class_mask)\n",
    "            loss1 = Network.other_loss_func(other_scores, self.m_neg, self.gamma)\n",
    "            loss2 = Network.class_loss_func(class_scores, class_label, self.m_neg, self.m_pos, self.gamma)\n",
    "            loss_sum = tf.reduce_sum(tf.concat([loss1,loss2], axis=0))\n",
    "            theta_loss = tf.keras.regularizers.l2(self.beta)(self.word_embed) + tf.keras.regularizers.l2(self.beta)(self.pos1_embed) + tf.keras.regularizers.l2(self.beta)(self.pos2_embed)\\\n",
    "                + tf.keras.regularizers.l2(self.beta)(self.class_matrix) + tf.keras.regularizers.l2(self.beta)(self.conv1.weights[0])\n",
    "            loss_sum = theta_loss + loss_sum\n",
    "            loss_avg = loss_sum / tf.cast(tf.shape(label)[0], tf.float32)\n",
    "            return x, loss_avg\n",
    "        else:\n",
    "            return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dim = 70\n",
    "class_dim = 1000\n",
    "hyperparams={\n",
    "    'm_neg':0.5,\n",
    "    'm_pos':3.,\n",
    "    'gamma':2.0,\n",
    "    'beta':0.001\n",
    "}\n",
    "network = Network(word_embed, pos_dim, WORD_DIM, RELATION_NUM-1, class_dim, OTHER_LABEL, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def train_func(dataset):\n",
    "    step_count = 0\n",
    "    loss_count = 0\n",
    "    for step, (pos1, pos2, label, words) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            scores,loss = network([pos1, pos2, label, words], training=True)\n",
    "        grads = tape.gradient(loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "        loss_count += loss\n",
    "        step_count += 1\n",
    "    loss_avg = loss_count / step_count\n",
    "    return loss_avg\n",
    "\n",
    "def test_func(dataset):\n",
    "    step_count = 0\n",
    "    pred_all = []\n",
    "    label_all = []\n",
    "    correct = 0\n",
    "    for step, (pos1, pos2, label, words) in enumerate(dataset):\n",
    "        scores = network([pos1, pos2, label, words], training=False)\n",
    "        pred = []\n",
    "        for item in scores:\n",
    "            if tf.reduce_max(item) < 0:\n",
    "                pred.append(OTHER_LABEL)\n",
    "            else:\n",
    "                pred.append(tf.argmax(item).numpy())\n",
    "        label_all.extend(list(label.numpy()))\n",
    "        pred_all.extend(pred)\n",
    "    for p, t in zip(pred_all, label_all):\n",
    "        if p == t:\n",
    "            correct += 1\n",
    "            \n",
    "    accuracy = correct / len(label_all)\n",
    "    return accuracy, pred_all, label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/runqi/.conda/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "training loss is  16.7153, training accuracy is  0.6823, test accuracy is  0.6194\n",
      "training loss is  12.5337, training accuracy is  0.7883, test accuracy is  0.7052\n",
      "training loss is  10.2890, training accuracy is  0.8946, test accuracy is  0.7413\n",
      "training loss is  8.6682, training accuracy is  0.9477, test accuracy is  0.7563\n",
      "training loss is  7.4365, training accuracy is  0.9808, test accuracy is  0.7622\n",
      "training loss is  6.5613, training accuracy is  0.9940, test accuracy is  0.7641\n",
      "training loss is  5.9435, training accuracy is  0.9989, test accuracy is  0.7656\n",
      "training loss is  5.4464, training accuracy is  0.9998, test accuracy is  0.7622\n",
      "training loss is  5.0255, training accuracy is  1.0000, test accuracy is  0.7652\n",
      "training loss is  4.6547, training accuracy is  1.0000, test accuracy is  0.7597\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "save_dir = './saved/'\n",
    "for epoch in range(10):\n",
    "    loss_avg = train_func(train_ds)\n",
    "    train_accuracy, pred_all, label_all = test_func(train_ds)\n",
    "    test_accuracy, pred_all, label_all = test_func(test_ds)\n",
    "    if test_accuracy > best_acc:\n",
    "        network.save_weights(save_dir + 'bestckpt')\n",
    "        best_acc = test_accuracy\n",
    "    print('training loss is {0: .4f}, training accuracy is {1: .4f}, test accuracy is {2: .4f}'.format(loss_avg, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7655502392344498\n"
     ]
    }
   ],
   "source": [
    "network.load_weights(save_dir + 'bestckpt')\n",
    "accuracy, pred_all, label_all = test_func(test_ds)\n",
    "label_all = [item[0] for item in label_all]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "id2rel = pickle.load(open(base_dir + 'lb2rel.dict', 'rb'))\n",
    "unique_relations = pickle.load(open(base_dir + 'unique_relations', 'rb'))\n",
    "label_all = [id2rel[item] for item in label_all]\n",
    "pred_all = [id2rel[item] for item in pred_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Entity-Destination       0.85      0.90      0.88       292\n",
      "     Entity-Origin       0.83      0.78      0.81       258\n",
      " Content-Container       0.81      0.84      0.82       192\n",
      "     Message-Topic       0.76      0.94      0.84       261\n",
      "  Product-Producer       0.73      0.74      0.74       231\n",
      " Member-Collection       0.78      0.92      0.84       233\n",
      "      Cause-Effect       0.91      0.90      0.90       328\n",
      " Instrument-Agency       0.71      0.73      0.72       156\n",
      "   Component-Whole       0.86      0.75      0.80       312\n",
      "\n",
      "         micro avg       0.81      0.84      0.83      2263\n",
      "         macro avg       0.80      0.83      0.82      2263\n",
      "      weighted avg       0.82      0.84      0.83      2263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label_all, pred_all, labels = [item for item in unique_relations if item !='Other']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
